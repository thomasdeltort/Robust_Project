{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pessimistic and Optimistic Bound on the Robustness Radius"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we try to study the different bounds of the robustness radius given by several methods (lip certificate, adv attacks, CROWN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from deel.lip.layers import (\n",
    "    SpectralDense,\n",
    "    SpectralConv2D,\n",
    "    ScaledL2NormPooling2D,\n",
    "    FrobeniusDense,\n",
    ")\n",
    "from keras.models import Sequential\n",
    "# from deel.lip.model import Sequential\n",
    "from deel.lip.activations import GroupSort\n",
    "from deel.lip.losses import MulticlassHKR, MulticlassKR\n",
    "from keras.layers import Input, Flatten, Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import keras.ops as K\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MNIST08 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the dataset has ten classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we select the two classes\n",
    "selected_classes = [0, 8]  # must be two classes as we perform binary classification\n",
    "\n",
    "\n",
    "def prepare_data(x, y, class_a=0, class_b=8):\n",
    "    \"\"\"\n",
    "    This function convert the MNIST data to make it suitable for our binary classification\n",
    "    setup.\n",
    "    \"\"\"\n",
    "    # select items from the two selected classes\n",
    "    mask = (y == class_a) + (\n",
    "        y == class_b\n",
    "    )  # mask to select only items from class_a or class_b\n",
    "    x = x[mask]\n",
    "    y = y[mask]\n",
    "    x = x.astype(\"float32\")\n",
    "    y = y.astype(\"float32\")\n",
    "    # convert from range int[0,255] to float32[-1,1]\n",
    "    x /= 255\n",
    "    x = x.reshape((-1, 28, 28, 1))\n",
    "    # change label to binary classification {-1,1}\n",
    "    y[y == class_a] = 1.0\n",
    "    y[y == class_b] = 0.0\n",
    "    return x, y.reshape((-1, 1))\n",
    "\n",
    "\n",
    "# now we load the dataset\n",
    "(x_train, y_train_ord), (x_test, y_test_ord) = mnist.load_data()\n",
    "# prepare the data\n",
    "x_train, y_train = prepare_data(\n",
    "    x_train, y_train_ord, selected_classes[0], selected_classes[1]\n",
    ")\n",
    "x_test, y_test = prepare_data(\n",
    "    x_test, y_test_ord, selected_classes[0], selected_classes[1]\n",
    ")\n",
    "y_test_ord = y_test[:,0]\n",
    "y_train_ord = y_train[:,0]\n",
    "y_test = to_categorical(y_test)\n",
    "y_train = to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.transpose(x_train,(0,3,1,2))\n",
    "x_test = np.transpose(x_test,(0,3,1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"lipModel\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"lipModel\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spectral_dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">25,120</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spectral_dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spectral_dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spectral_dense (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │        \u001b[38;5;34m25,120\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spectral_dense_1 (\u001b[38;5;33mDense\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m528\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spectral_dense_2 (\u001b[38;5;33mDense\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │            \u001b[38;5;34m32\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,680</span> (100.31 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m25,680\u001b[0m (100.31 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,680</span> (100.31 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m25,680\u001b[0m (100.31 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vanilla_model = keras.models.load_model(\"/home/aws_install/robustess_project/lip_models/demo3_FC_vanilla_MNIST08_channelfirst_False_disj_Neurons.keras\")\n",
    "vanilla_model.compile(\n",
    "    # decreasing alpha and increasing min_margin improve robustness (at the cost of accuracy)\n",
    "    # note also in the case of lipschitz networks, more robustness require more parameters.\n",
    "    loss=MulticlassHKR(alpha=100, min_margin=0.25),\n",
    "    optimizer=Adam(1e-4),\n",
    "    metrics=[\"accuracy\", MulticlassKR()],)\n",
    "vanilla_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = vanilla_model.layers[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dense = Dense(units=4, activation=None, use_bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_model_bis = Sequential(vanilla_model.layers[:-1] + [new_dense])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor shape=(None, 4), dtype=float32, sparse=False, ragged=False, name=keras_tensor_26>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dense(layer.input) # compile and erase weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_temp = np.zeros((16,4), dtype = 'float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_temp = np.zeros((4,))\n",
    "b_temp[2:] = -10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = layer.get_weights()[0] #(16,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 2)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_temp[:,:2] = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dense.set_weights([w_temp,b_temp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spectral_dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">25,120</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spectral_dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">68</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spectral_dense (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │        \u001b[38;5;34m25,120\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spectral_dense_1 (\u001b[38;5;33mDense\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m528\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │            \u001b[38;5;34m68\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,716</span> (100.45 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m25,716\u001b[0m (100.45 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,716</span> (100.45 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m25,716\u001b[0m (100.45 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vanilla_model_bis.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vanilla_model_bis.layers[-1].get_weights()[0]-w_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-2.5456893e+00,  2.5312362e+00, -1.0000000e+04, -1.0000000e+04]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vanilla_model_bis.predict(x_test[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-2.5456893,  2.5312362]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vanilla_model.predict(x_test[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# strategy: first\n",
    "# we select a sample from each class.\n",
    "images_list = []\n",
    "labels_list = []\n",
    "# select only a few element from the test set\n",
    "# selected = np.random.choice(len(y_test_ord), 500)\n",
    "sub_y_test_ord = y_test_ord[:400]\n",
    "sub_x_test = x_test[:400]\n",
    "# drop misclassified elements\n",
    "misclassified_mask = K.equal(\n",
    "    K.argmax(vanilla_model.predict(sub_x_test, verbose=0), axis=-1), sub_y_test_ord\n",
    ")\n",
    "sub_x_test = sub_x_test[misclassified_mask.detach().cpu().numpy()]\n",
    "sub_y_test_ord = sub_y_test_ord[misclassified_mask.detach().cpu().numpy()]\n",
    "for i in range(2):\n",
    "    # select the 20 firsts elements of the ith label\n",
    "    label_mask = sub_y_test_ord == i\n",
    "    x = sub_x_test[label_mask][:100]\n",
    "    y = sub_y_test_ord[label_mask][:100]\n",
    "    # convert it to tensor for use with foolbox\n",
    "    images = K.convert_to_tensor(x.astype(\"float32\"), dtype=\"float32\")\n",
    "    labels = K.convert_to_tensor(y, dtype=\"int64\")\n",
    "    # repeat the input 10 times, one per misclassification target\n",
    "    for j in range(100):\n",
    "        images_list.append(images[j])\n",
    "        labels_list.append(labels[j])\n",
    "images = K.convert_to_tensor(images_list)\n",
    "labels = K.convert_to_tensor(labels_list)\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "images shape = (nombre classes, nbr de points , channels, dim img 1, dim img 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Pessimistic Radius via Lip Constraints (l2 norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_binary_certificate(images, model, L=1):    \n",
    "#     values = model(images)[:,0]\n",
    "#     certificates = np.abs(values.detach().cpu().numpy())/L\n",
    "#     return certificates   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_certificate(images, model, L=1):    \n",
    "    values, _ = K.top_k(model(images), k=2)\n",
    "    certificates = (values[:, 0] - values[:, 1]) / (np.sqrt(2)*L)\n",
    "    return certificates.detach().cpu().numpy()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lip_radius = compute_certificate(images, vanilla_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.8217466, 2.0785058, 2.117814 , 3.9895344, 3.5977595, 3.214799 ,\n",
       "       2.146869 , 2.9655879, 3.3513803, 1.6075324], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lip_radius[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Optimistic Radius via AutoAttacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aws_install/miniconda3/envs/k3torchenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import keras.ops as K\n",
    "import matplotlib.pyplot as plt\n",
    "import torchattacks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchattacks\n",
    "from robustbench.utils import clean_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.4315052], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lip_radius[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29554/1194193214.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  var = torch.tensor(images[:1].cuda(), requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "var = torch.tensor(images[:1].cuda(), requires_grad=True)\n",
    "\n",
    "y = vanilla_model(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_output = y[0, 0]\n",
    "select_output.backward()\n",
    "gradient_vanilla = var.grad.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29554/1007859780.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  var_bis = torch.tensor(images[:1].cuda(), requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "var_bis = torch.tensor(images[:1].cuda(), requires_grad=True)\n",
    "\n",
    "y_bis = vanilla_model_bis(var_bis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_output = y_bis[0, 0]\n",
    "select_output.backward()\n",
    "gradient_bis = var_bis.grad.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_bis-gradient_vanilla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FGSM ne donne pas la même attaque sur les deux modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atk = torchattacks.FGSM(vanilla_model,eps=1)\n",
    "adv_image_bis1 = atk(images[:1], labels[:1])\n",
    "\n",
    "atk = torchattacks.FGSM(vanilla_model_bis,eps=1)\n",
    "adv_image_bis2 = atk(images[:1], labels[:1])\n",
    "\n",
    "(adv_image_bis2 - adv_image_bis1).min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pareil PGD l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atk = torchattacks.PGDL2(vanilla_model_bis, eps=2, alpha=2/5, steps=10, random_start=False)\n",
    "adv_image_bis1 = atk(images[:1], labels[:1])\n",
    "\n",
    "atk = torchattacks.PGDL2(vanilla_model, eps=2, alpha=2/5, steps=10, random_start=False)\n",
    "adv_image_bis2 = atk(images[:1], labels[:1])\n",
    "\n",
    "(adv_image_bis2 - adv_image_bis1).min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention quand on vient chercher si l'attaque a fonctionné il faut faire argmax sur les deux premiers neuronnes uniquement !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False], device='cuda:0')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.argmax(vanilla_model(adv_image_bis1)[:2], axis=1) == labels[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.4315052], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lip_radius[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------- running L2-attack with epsilon 1.5210 --------------------------\n",
      "initial accuracy: 100.00%\n",
      "parameters:  10 2 1 1\n",
      "iteration: 0 - Best loss: 0.146943\n",
      "iteration: 1 - Best loss: 0.593290\n",
      "iteration: 2 - Best loss: 0.689178\n",
      "iteration: 3 - Best loss: 0.689902\n",
      "iteration: 4 - Best loss: 0.689902\n",
      "iteration: 5 - Best loss: 0.689902\n",
      "iteration: 6 - Best loss: 0.689902\n",
      "iteration: 7 - Best loss: 0.689902\n",
      "iteration: 8 - Best loss: 0.689902\n",
      "iteration: 9 - Best loss: 0.689902\n",
      "restart 0 - robust accuracy: 100.00% - cum. time: 0.1 s\n",
      "-------------------------- running L2-attack with epsilon 1.5210 --------------------------\n",
      "initial accuracy: 100.00%\n",
      "parameters:  10 2 1 1\n",
      "iteration: 0 - Best loss: -0.000184\n",
      "iteration: 1 - Best loss: -0.000021\n",
      "iteration: 2 - Best loss: -0.000001\n",
      "iteration: 3 - Best loss: -0.000000\n",
      "iteration: 4 - Best loss: -0.000000\n",
      "iteration: 5 - Best loss: -0.000000\n",
      "iteration: 6 - Best loss: -0.000000\n",
      "iteration: 7 - Best loss: -0.000000\n",
      "iteration: 8 - Best loss: -0.000000\n",
      "iteration: 9 - Best loss: -0.000000\n",
      "restart 0 - target_class 2 - robust accuracy: 100.00% at eps = 1.52100 - cum. time: 0.1 s\n",
      "Clean accuracy: 100.00%\n",
      "success rate: 1/1 (on correctly classified points) in 0.1 s\n",
      "restart 0 - target_class 2 - robust accuracy: 100.00% at eps = 1.52100 - cum. time: 0.1 s\n",
      "restart 0 - robust accuracy: 100.00% - cum. time: 24.2 s\n"
     ]
    }
   ],
   "source": [
    "atk = torchattacks.AutoAttack(vanilla_model_bis, norm='L2', eps=1.521, version=\"standard\", n_classes=2, verbose=True, seed=1)\n",
    "adv_image_2 = atk(images[:1], labels[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------- running L2-attack with epsilon 1.5220 --------------------------\n",
      "initial accuracy: 100.00%\n",
      "parameters:  10 2 1 1\n",
      "iteration: 0 - Best loss: 0.146959\n",
      "iteration: 1 - Best loss: 0.593803\n",
      "iteration: 2 - Best loss: 0.689830\n",
      "iteration: 3 - Best loss: 0.690554\n",
      "iteration: 4 - Best loss: 0.690554\n",
      "iteration: 5 - Best loss: 0.690554\n",
      "iteration: 6 - Best loss: 0.690554\n",
      "iteration: 7 - Best loss: 0.690554\n",
      "iteration: 8 - Best loss: 0.690554\n",
      "iteration: 9 - Best loss: 0.690554\n",
      "restart 0 - robust accuracy: 100.00% - cum. time: 0.0 s\n",
      "-------------------------- running L2-attack with epsilon 1.5220 --------------------------\n",
      "initial accuracy: 100.00%\n",
      "parameters:  10 2 1 1\n",
      "iteration: 0 - Best loss: -0.000184\n",
      "iteration: 1 - Best loss: -0.000021\n",
      "iteration: 2 - Best loss: -0.000001\n",
      "iteration: 3 - Best loss: 0.000000\n",
      "iteration: 4 - Best loss: 0.000000\n",
      "iteration: 5 - Best loss: 0.000000\n",
      "iteration: 6 - Best loss: 0.000000\n",
      "iteration: 7 - Best loss: 0.000000\n",
      "iteration: 8 - Best loss: 0.000000\n",
      "iteration: 9 - Best loss: 0.000000\n",
      "restart 0 - target_class 2 - robust accuracy: 0.00% at eps = 1.52200 - cum. time: 0.1 s\n"
     ]
    }
   ],
   "source": [
    "atk = torchattacks.AutoAttack(vanilla_model_bis, norm='L2', eps=1.522, version=\"standard\", n_classes=4, verbose=True, seed=1)\n",
    "adv_image_4 = atk(images[:1], labels[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2026, device='cuda:0', grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(adv_image_4 - adv_image_2).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1], device='cuda:0', dtype=torch.int32)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.argmax(vanilla_model(adv_image_2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False], device='cuda:0')"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.argmax(vanilla_model(adv_image_4), axis=1) == labels[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0], device='cuda:0')"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9.8077e-01, -1.0437e+00, -1.0000e+04, -1.0000e+04]], device='cuda:0',\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vanilla_model_bis(adv_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2], device='cuda:0', dtype=torch.int32)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.argmax(vanilla_model_bis(adv_image), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_compute_optimistic_radius_PGD(image, target, certificate, model, n_iter = 10):\n",
    "\n",
    "    # we find a multiple of the certificate such as we find an adversarial attack\n",
    "    max_iter = 30\n",
    "    optimistic_radius = 0\n",
    "    k=0\n",
    "    while (optimistic_radius == 0):\n",
    "        k = k+1\n",
    "        eps_current = k*certificate[0]\n",
    "        atk_van = torchattacks.PGDL2(model, eps=eps_current, alpha=eps_current/5, steps=10, random_start=True)\n",
    "        adv_image = atk_van(image, target)\n",
    "        # return 0 if the attack doesn't work\n",
    "        optimistic_radius = (image - adv_image).square().sum(dim=(1, 2, 3)).sqrt() if (keras.ops.argmax(vanilla_model(adv_image), axis=1) != target) else 0\n",
    "    print(\"We found an adversarial attack for eps=\", k*certificate)\n",
    "    initial_eps = k*certificate[0]\n",
    "    # We use dichotomy algorithm to fine the smallest optimistic radius\n",
    "    print(\"Launching Dichotomy :\")\n",
    "    d_up = k*certificate[0]\n",
    "    d_low = 0\n",
    "    attack_succeeded = False\n",
    "    i= 0\n",
    "    while (i<n_iter or attack_succeeded == False) and i<max_iter:\n",
    "        eps_current = (d_up+d_low)/2\n",
    "        atk_van = torchattacks.PGDL2(model, eps=eps_current, alpha=eps_current/5, steps=10, random_start=True)\n",
    "        adv_image = atk_van(image, target)\n",
    "        # return 0 if the attack doesn't work\n",
    "        optimistic_radius = (image - adv_image).square().sum(dim=(1, 2, 3)).sqrt() if (keras.ops.argmax(vanilla_model(adv_image), axis=1) != target) else 0\n",
    "        if optimistic_radius == 0:\n",
    "            attack_succeeded = False\n",
    "            d_low = eps_current\n",
    "        else:\n",
    "            attack_succeeded = True\n",
    "            d_up = eps_current\n",
    "        print(i, eps_current, attack_succeeded)\n",
    "        i = i+1\n",
    "    # traitement non convergence\n",
    "    if i==max_iter:\n",
    "        eps_current = initial_eps\n",
    "    return eps_current\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We found an adversarial attack for eps= [5.998551]\n",
      "Launching Dichotomy :\n",
      "0 2.9992754 False\n",
      "1 4.4989133 True\n",
      "2 3.7490945 True\n",
      "3 3.374185 True\n",
      "4 3.1867304 True\n",
      "5 3.0930028 False\n",
      "6 3.1398666 False\n",
      "7 3.1632986 True\n",
      "8 3.1515827 True\n",
      "9 3.1457248 True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float32(3.1457248)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_compute_optimistic_radius_PGD(images[2:3], labels[2:3], lip_radius[2:3], vanilla_model, n_iter = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_compute_optimistic_radius_AA(image, target, certificate, model, n_iter = 10):\n",
    "    # we find a multiple of the certificate such as we find an adversarial attack\n",
    "    optimistic_radius = 0\n",
    "    k=0\n",
    "    while (optimistic_radius == 0):\n",
    "        k = k+1\n",
    "        eps_current = k*certificate[0]\n",
    "        atk = torchattacks.AutoAttack(model, norm='L2', eps=eps_current, version=\"standard\", n_classes=2)\n",
    "        print(type(atk._autoattack))\n",
    "        atk._autoattack.attacks.pop(1)\n",
    "        print(atk._autoattack.attacks)\n",
    "        # print(type(atk._attacks))\n",
    "        # atk.apgd_t = False  # Désactive APGD targeted\n",
    "        # atk.apgd_ce = True  # Active APGD Cross-Entropy\n",
    "        # atk.apgd_mt = False  # Désactiver APGD Multi-targeted\n",
    "        atk = torchattacks.AutoAttack(model, norm='L2', eps=eps_current, version=\"standard\", n_classes=2)\n",
    "        adv_image = atk(image, target)\n",
    "        optimistic_radius = (image - adv_image).square().sum(dim=(1, 2, 3)).sqrt()\n",
    "        print(optimistic_radius)\n",
    "    print(\"We found an adversarial attack for eps=\", k*certificate)\n",
    "\n",
    "    # We use dichotomy algorithm to fine the smallest optimistic radius\n",
    "    print(\"Launching Dichotomy :\")\n",
    "    d_up = k*certificate[0]\n",
    "    d_low = 0\n",
    "    attack_succeeded = False\n",
    "    i= 0\n",
    "    while (i<n_iter or attack_succeeded == False):\n",
    "        eps_current = (d_up+d_low)/2\n",
    "        atk = torchattacks.AutoAttack(model, norm='L2', eps=eps_current)\n",
    "        adv_image = atk(image, target)\n",
    "        optimistic_radius = (image - adv_image).square().sum(dim=(1, 2, 3)).sqrt()\n",
    "        if optimistic_radius == 0:\n",
    "            attack_succeeded = False\n",
    "            d_low = eps_current\n",
    "        else:\n",
    "            attack_succeeded = True\n",
    "            d_up = eps_current\n",
    "        print(i, eps_current, attack_succeeded)\n",
    "        i = i+1\n",
    "    return eps_current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchattacks.wrappers.multiattack.MultiAttack'>\n",
      "[APGD(model_name=Sequential, device=cuda:0, attack_mode=default, targeted=False, normalization_used=False, eps=2.9992754459381104, steps=10, norm=L2, n_restarts=1, seed=1744118861.6536021, loss=ce, eot_iter=1, thr_decr=0.75, verbose=False), FAB(model_name=Sequential, device=cuda:0, attack_mode=default, targeted=False, normalization_used=False, norm=L2, n_restarts=1, eps=2.9992754459381104, alpha_max=0.1, eta=1.05, beta=0.9, steps=10, verbose=False, seed=1744118861.6536927, target_class=None, multi_targeted=True, n_target_classes=1), Square(model_name=Sequential, device=cuda:0, attack_mode=default, targeted=False, normalization_used=False, norm=L2, n_queries=5000, eps=2.9992754459381104, p_init=0.8, n_restarts=1, seed=1744118861.6537397, verbose=False, loss=margin, rescale_schedule=True)]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index -3 is out of bounds for dimension 1 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msingle_compute_optimistic_radius_AA\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlip_radius\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvanilla_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[90], line 17\u001b[0m, in \u001b[0;36msingle_compute_optimistic_radius_AA\u001b[0;34m(image, target, certificate, model, n_iter)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# print(type(atk._attacks))\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# atk.apgd_t = False  # Désactive APGD targeted\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# atk.apgd_ce = True  # Active APGD Cross-Entropy\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# atk.apgd_mt = False  # Désactiver APGD Multi-targeted\u001b[39;00m\n\u001b[1;32m     16\u001b[0m atk \u001b[38;5;241m=\u001b[39m torchattacks\u001b[38;5;241m.\u001b[39mAutoAttack(model, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL2\u001b[39m\u001b[38;5;124m'\u001b[39m, eps\u001b[38;5;241m=\u001b[39meps_current, version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstandard\u001b[39m\u001b[38;5;124m\"\u001b[39m, n_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m adv_image \u001b[38;5;241m=\u001b[39m \u001b[43matk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m optimistic_radius \u001b[38;5;241m=\u001b[39m (image \u001b[38;5;241m-\u001b[39m adv_image)\u001b[38;5;241m.\u001b[39msquare()\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m))\u001b[38;5;241m.\u001b[39msqrt()\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(optimistic_radius)\n",
      "File \u001b[0;32m~/miniconda3/envs/k3torchenv/lib/python3.10/site-packages/torchattacks/attack.py:511\u001b[0m, in \u001b[0;36mAttack.__call__\u001b[0;34m(self, inputs, labels, *args, **kwargs)\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_normalization_applied(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m     adv_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;66;03m# adv_inputs = self.to_type(adv_inputs, self.return_type)\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recover_model_mode(given_training)\n",
      "File \u001b[0;32m~/miniconda3/envs/k3torchenv/lib/python3.10/site-packages/torchattacks/attacks/autoattack.py:199\u001b[0m, in \u001b[0;36mAutoAttack.forward\u001b[0;34m(self, images, labels)\u001b[0m\n\u001b[1;32m    197\u001b[0m images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    198\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 199\u001b[0m adv_images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autoattack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m adv_images\n",
      "File \u001b[0;32m~/miniconda3/envs/k3torchenv/lib/python3.10/site-packages/torchattacks/attack.py:511\u001b[0m, in \u001b[0;36mAttack.__call__\u001b[0;34m(self, inputs, labels, *args, **kwargs)\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_normalization_applied(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m     adv_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;66;03m# adv_inputs = self.to_type(adv_inputs, self.return_type)\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recover_model_mode(given_training)\n",
      "File \u001b[0;32m~/miniconda3/envs/k3torchenv/lib/python3.10/site-packages/torchattacks/wrappers/multiattack.py:55\u001b[0m, in \u001b[0;36mMultiAttack.forward\u001b[0;34m(self, images, labels)\u001b[0m\n\u001b[1;32m     52\u001b[0m multi_atk_records \u001b[38;5;241m=\u001b[39m [batch_size]\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, attack \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattacks):\n\u001b[0;32m---> 55\u001b[0m     adv_images \u001b[38;5;241m=\u001b[39m \u001b[43mattack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfails\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfails\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_logits(adv_images)\n\u001b[1;32m     58\u001b[0m     _, pre \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/k3torchenv/lib/python3.10/site-packages/torchattacks/attack.py:511\u001b[0m, in \u001b[0;36mAttack.__call__\u001b[0;34m(self, inputs, labels, *args, **kwargs)\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_normalization_applied(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m     adv_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;66;03m# adv_inputs = self.to_type(adv_inputs, self.return_type)\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recover_model_mode(given_training)\n",
      "File \u001b[0;32m~/miniconda3/envs/k3torchenv/lib/python3.10/site-packages/torchattacks/attacks/apgdt.py:75\u001b[0m, in \u001b[0;36mAPGDT.forward\u001b[0;34m(self, images, labels)\u001b[0m\n\u001b[1;32m     73\u001b[0m images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     74\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 75\u001b[0m _, adv_images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperturb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m adv_images\n",
      "File \u001b[0;32m~/miniconda3/envs/k3torchenv/lib/python3.10/site-packages/torchattacks/attacks/apgdt.py:343\u001b[0m, in \u001b[0;36mAPGDT.perturb\u001b[0;34m(self, x_in, y_in, best_loss, cheap)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ind_to_fool\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    334\u001b[0m     x_to_fool, y_to_fool \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    335\u001b[0m         x[ind_to_fool]\u001b[38;5;241m.\u001b[39mclone(),\n\u001b[1;32m    336\u001b[0m         y[ind_to_fool]\u001b[38;5;241m.\u001b[39mclone(),\n\u001b[1;32m    337\u001b[0m     )  \u001b[38;5;66;03m# nopep8\u001b[39;00m\n\u001b[1;32m    338\u001b[0m     (\n\u001b[1;32m    339\u001b[0m         best_curr,\n\u001b[1;32m    340\u001b[0m         acc_curr,\n\u001b[1;32m    341\u001b[0m         loss_curr,\n\u001b[1;32m    342\u001b[0m         adv_curr,\n\u001b[0;32m--> 343\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattack_single_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx_to_fool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_to_fool\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# nopep8\u001b[39;00m\n\u001b[1;32m    346\u001b[0m     ind_curr \u001b[38;5;241m=\u001b[39m (acc_curr \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mnonzero()\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m    348\u001b[0m     acc[ind_to_fool[ind_curr]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/k3torchenv/lib/python3.10/site-packages/torchattacks/attacks/apgdt.py:143\u001b[0m, in \u001b[0;36mAPGDT.attack_single_run\u001b[0;34m(self, x_in, y_in)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;66;03m# 1 forward pass (eot_iter = 1)\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_logits(x_adv)\n\u001b[0;32m--> 143\u001b[0m     loss_indiv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdlr_loss_targeted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_target\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_indiv\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# 1 backward pass (eot_iter = 1)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/k3torchenv/lib/python3.10/site-packages/torchattacks/attacks/apgdt.py:93\u001b[0m, in \u001b[0;36mAPGDT.dlr_loss_targeted\u001b[0;34m(self, x, y, y_target)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdlr_loss_targeted\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, y, y_target):\n\u001b[1;32m     90\u001b[0m     x_sorted, ind_sorted \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msort(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m(x[np\u001b[38;5;241m.\u001b[39marange(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]), y] \u001b[38;5;241m-\u001b[39m x[np\u001b[38;5;241m.\u001b[39marange(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]), y_target]) \u001b[38;5;241m/\u001b[39m (\n\u001b[0;32m---> 93\u001b[0m         x_sorted[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[43mx_sorted\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m x_sorted[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-12\u001b[39m\n\u001b[1;32m     94\u001b[0m     )\n",
      "\u001b[0;31mIndexError\u001b[0m: index -3 is out of bounds for dimension 1 with size 2"
     ]
    }
   ],
   "source": [
    "single_compute_optimistic_radius_AA(images[2:3], labels[2:3], lip_radius[2:3], vanilla_model, n_iter = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_PGD = []\n",
    "eps_AA = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index -3 is out of bounds for dimension 1 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[135], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     eps_AA\u001b[38;5;241m.\u001b[39mappend(\u001b[43msingle_compute_optimistic_radius_AA\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlip_radius\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvanilla_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[133], line 14\u001b[0m, in \u001b[0;36msingle_compute_optimistic_radius_AA\u001b[0;34m(image, target, certificate, model, n_iter)\u001b[0m\n\u001b[1;32m      9\u001b[0m atk\u001b[38;5;241m.\u001b[39mattacks_to_run \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapgd-ce\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Forcer uniquement APGD Cross-Entropy\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# atk.apgd_t = False  # Désactive APGD targeted\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# atk.apgd_ce = True  # Active APGD Cross-Entropy\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# atk.apgd_mt = False  # Désactiver APGD Multi-targeted\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m adv_image \u001b[38;5;241m=\u001b[39m \u001b[43matk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m optimistic_radius \u001b[38;5;241m=\u001b[39m (image \u001b[38;5;241m-\u001b[39m adv_image)\u001b[38;5;241m.\u001b[39msquare()\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m))\u001b[38;5;241m.\u001b[39msqrt()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(optimistic_radius)\n",
      "File \u001b[0;32m~/miniconda3/envs/k3torchenv/lib/python3.10/site-packages/torchattacks/attack.py:511\u001b[0m, in \u001b[0;36mAttack.__call__\u001b[0;34m(self, inputs, labels, *args, **kwargs)\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_normalization_applied(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m     adv_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;66;03m# adv_inputs = self.to_type(adv_inputs, self.return_type)\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recover_model_mode(given_training)\n",
      "File \u001b[0;32m~/miniconda3/envs/k3torchenv/lib/python3.10/site-packages/torchattacks/attacks/autoattack.py:199\u001b[0m, in \u001b[0;36mAutoAttack.forward\u001b[0;34m(self, images, labels)\u001b[0m\n\u001b[1;32m    197\u001b[0m images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    198\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 199\u001b[0m adv_images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autoattack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m adv_images\n",
      "File \u001b[0;32m~/miniconda3/envs/k3torchenv/lib/python3.10/site-packages/torchattacks/attack.py:511\u001b[0m, in \u001b[0;36mAttack.__call__\u001b[0;34m(self, inputs, labels, *args, **kwargs)\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_normalization_applied(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m     adv_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;66;03m# adv_inputs = self.to_type(adv_inputs, self.return_type)\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recover_model_mode(given_training)\n",
      "File \u001b[0;32m~/miniconda3/envs/k3torchenv/lib/python3.10/site-packages/torchattacks/wrappers/multiattack.py:55\u001b[0m, in \u001b[0;36mMultiAttack.forward\u001b[0;34m(self, images, labels)\u001b[0m\n\u001b[1;32m     52\u001b[0m multi_atk_records \u001b[38;5;241m=\u001b[39m [batch_size]\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, attack \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattacks):\n\u001b[0;32m---> 55\u001b[0m     adv_images \u001b[38;5;241m=\u001b[39m \u001b[43mattack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfails\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfails\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_logits(adv_images)\n\u001b[1;32m     58\u001b[0m     _, pre \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/k3torchenv/lib/python3.10/site-packages/torchattacks/attack.py:511\u001b[0m, in \u001b[0;36mAttack.__call__\u001b[0;34m(self, inputs, labels, *args, **kwargs)\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_normalization_applied(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m     adv_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;66;03m# adv_inputs = self.to_type(adv_inputs, self.return_type)\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recover_model_mode(given_training)\n",
      "File \u001b[0;32m~/miniconda3/envs/k3torchenv/lib/python3.10/site-packages/torchattacks/attacks/apgdt.py:75\u001b[0m, in \u001b[0;36mAPGDT.forward\u001b[0;34m(self, images, labels)\u001b[0m\n\u001b[1;32m     73\u001b[0m images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     74\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 75\u001b[0m _, adv_images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperturb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m adv_images\n",
      "File \u001b[0;32m~/miniconda3/envs/k3torchenv/lib/python3.10/site-packages/torchattacks/attacks/apgdt.py:343\u001b[0m, in \u001b[0;36mAPGDT.perturb\u001b[0;34m(self, x_in, y_in, best_loss, cheap)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ind_to_fool\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    334\u001b[0m     x_to_fool, y_to_fool \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    335\u001b[0m         x[ind_to_fool]\u001b[38;5;241m.\u001b[39mclone(),\n\u001b[1;32m    336\u001b[0m         y[ind_to_fool]\u001b[38;5;241m.\u001b[39mclone(),\n\u001b[1;32m    337\u001b[0m     )  \u001b[38;5;66;03m# nopep8\u001b[39;00m\n\u001b[1;32m    338\u001b[0m     (\n\u001b[1;32m    339\u001b[0m         best_curr,\n\u001b[1;32m    340\u001b[0m         acc_curr,\n\u001b[1;32m    341\u001b[0m         loss_curr,\n\u001b[1;32m    342\u001b[0m         adv_curr,\n\u001b[0;32m--> 343\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattack_single_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx_to_fool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_to_fool\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# nopep8\u001b[39;00m\n\u001b[1;32m    346\u001b[0m     ind_curr \u001b[38;5;241m=\u001b[39m (acc_curr \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mnonzero()\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m    348\u001b[0m     acc[ind_to_fool[ind_curr]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/k3torchenv/lib/python3.10/site-packages/torchattacks/attacks/apgdt.py:143\u001b[0m, in \u001b[0;36mAPGDT.attack_single_run\u001b[0;34m(self, x_in, y_in)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;66;03m# 1 forward pass (eot_iter = 1)\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_logits(x_adv)\n\u001b[0;32m--> 143\u001b[0m     loss_indiv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdlr_loss_targeted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_target\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_indiv\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# 1 backward pass (eot_iter = 1)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/k3torchenv/lib/python3.10/site-packages/torchattacks/attacks/apgdt.py:93\u001b[0m, in \u001b[0;36mAPGDT.dlr_loss_targeted\u001b[0;34m(self, x, y, y_target)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdlr_loss_targeted\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, y, y_target):\n\u001b[1;32m     90\u001b[0m     x_sorted, ind_sorted \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msort(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m(x[np\u001b[38;5;241m.\u001b[39marange(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]), y] \u001b[38;5;241m-\u001b[39m x[np\u001b[38;5;241m.\u001b[39marange(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]), y_target]) \u001b[38;5;241m/\u001b[39m (\n\u001b[0;32m---> 93\u001b[0m         x_sorted[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[43mx_sorted\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m x_sorted[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-12\u001b[39m\n\u001b[1;32m     94\u001b[0m     )\n",
      "\u001b[0;31mIndexError\u001b[0m: index -3 is out of bounds for dimension 1 with size 2"
     ]
    }
   ],
   "source": [
    "for i in range(2,10):\n",
    "    eps_AA.append(single_compute_optimistic_radius_AA(images[i:i+1], labels[i:i+1], lip_radius[i:i+1], vanilla_model, n_iter = 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We found an adversarial attack for eps= [2.3915052]\n",
      "Launching Dichotomy :\n",
      "0 1.1957526 False\n",
      "1 1.7936289 False\n",
      "2 2.092567 True\n",
      "3 1.943098 False\n",
      "4 2.0178325 False\n",
      "5 2.0551996 True\n",
      "6 2.0365162 False\n",
      "7 2.045858 False\n",
      "8 2.0505288 True\n",
      "9 2.0481935 False\n",
      "10 2.0493612 True\n",
      "We found an adversarial attack for eps= [0.47378108]\n",
      "Launching Dichotomy :\n",
      "0 0.23689054 False\n",
      "1 0.3553358 False\n",
      "2 0.41455844 False\n",
      "3 0.44416976 False\n",
      "4 0.45897543 True\n",
      "5 0.4515726 True\n",
      "6 0.44787118 True\n",
      "7 0.44602048 True\n",
      "8 0.44509512 False\n",
      "9 0.4455578 False\n",
      "10 0.44578916 True\n",
      "We found an adversarial attack for eps= [1.2187829]\n",
      "Launching Dichotomy :\n",
      "0 0.60939145 False\n",
      "1 0.9140872 False\n",
      "2 1.0664351 False\n",
      "3 1.142609 False\n",
      "4 1.180696 False\n",
      "5 1.1997395 True\n",
      "6 1.1902177 True\n",
      "7 1.1854569 False\n",
      "8 1.1878374 False\n",
      "9 1.1890275 False\n",
      "10 1.1896226 False\n",
      "11 1.1899202 True\n",
      "We found an adversarial attack for eps= [1.2469541]\n",
      "Launching Dichotomy :\n",
      "0 0.62347704 False\n",
      "1 0.9352156 False\n",
      "2 1.0910848 True\n",
      "3 1.0131502 False\n",
      "4 1.0521176 False\n",
      "5 1.0716012 True\n",
      "6 1.0618594 False\n",
      "7 1.0667303 False\n",
      "8 1.0691657 False\n",
      "9 1.0703834 True\n",
      "We found an adversarial attack for eps= [1.7667823]\n",
      "Launching Dichotomy :\n",
      "0 0.88339114 False\n",
      "1 1.3250867 False\n",
      "2 1.5459344 False\n",
      "3 1.6563584 False\n",
      "4 1.7115703 False\n",
      "5 1.7391763 True\n",
      "6 1.7253733 True\n",
      "7 1.7184718 True\n",
      "8 1.715021 False\n",
      "9 1.7167463 True\n",
      "We found an adversarial attack for eps= [1.7735469]\n",
      "Launching Dichotomy :\n",
      "0 0.88677347 False\n",
      "1 1.3301601 False\n",
      "2 1.5518535 False\n",
      "3 1.6627002 False\n",
      "4 1.7181236 False\n",
      "5 1.7458353 False\n",
      "6 1.7596911 True\n",
      "7 1.7527633 True\n",
      "8 1.7492993 True\n",
      "9 1.7475673 True\n",
      "We found an adversarial attack for eps= [0.48722437]\n",
      "Launching Dichotomy :\n",
      "0 0.24361219 False\n",
      "1 0.3654183 False\n",
      "2 0.42632133 False\n",
      "3 0.45677286 True\n",
      "4 0.4415471 True\n",
      "5 0.4339342 False\n",
      "6 0.43774065 False\n",
      "7 0.43964386 False\n",
      "8 0.44059548 False\n",
      "9 0.44107127 False\n",
      "10 0.44130918 False\n",
      "11 0.44142812 False\n",
      "12 0.4414876 True\n",
      "We found an adversarial attack for eps= [1.9154077]\n",
      "Launching Dichotomy :\n",
      "0 0.9577038 False\n",
      "1 1.4365557 False\n",
      "2 1.6759818 False\n",
      "3 1.7956947 True\n",
      "4 1.7358382 True\n",
      "5 1.70591 True\n",
      "6 1.6909459 False\n",
      "7 1.6984279 True\n",
      "8 1.6946869 False\n",
      "9 1.6965574 True\n",
      "We found an adversarial attack for eps= [1.5327566]\n",
      "Launching Dichotomy :\n",
      "0 0.7663783 False\n",
      "1 1.1495674 False\n",
      "2 1.341162 False\n",
      "3 1.4369593 False\n",
      "4 1.4848579 True\n",
      "5 1.4609087 True\n",
      "6 1.448934 True\n",
      "7 1.4429467 True\n",
      "8 1.439953 True\n",
      "9 1.438456 True\n",
      "We found an adversarial attack for eps= [3.4818492]\n",
      "Launching Dichotomy :\n",
      "0 1.7409246 False\n",
      "1 2.6113868 False\n",
      "2 3.046618 False\n",
      "3 3.2642336 False\n",
      "4 3.3730414 False\n",
      "5 3.4274454 False\n",
      "6 3.4546473 False\n",
      "7 3.4682484 False\n",
      "8 3.4750488 False\n",
      "9 3.4784489 True\n"
     ]
    }
   ],
   "source": [
    "# for i in range(len(images)):\n",
    "for i in range(10):\n",
    "    eps_AA.append(single_compute_optimistic_radius_PGD(images[i:i+1], labels[i:i+1], lip_radius[i:i+1], vanilla_model, n_iter = 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image #     Certificate     Distance to adversarial\n",
      "---------------------------------------------------\n",
      "Image 0        0.342                2.01\n",
      "Image 1        0.068                0.45\n",
      "Image 2        0.203                2.05\n",
      "Image 3        0.178                0.45\n",
      "Image 4        0.294                2.02\n",
      "Image 5        0.296                0.46\n",
      "Image 6        0.070                1.21\n",
      "Image 7        0.274                1.08\n",
      "Image 8        0.219                1.71\n",
      "Image 9        0.580                1.72\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "print(\"Image #     Certificate     Distance to adversarial\")\n",
    "print(\"---------------------------------------------------\")\n",
    "for i in range(10):\n",
    "    print(f\"Image {i}        {lip_radius[i]:.3f}                {eps_AA[i]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.float32(2.0128279),\n",
       " np.float32(0.44785672),\n",
       " np.float32(2.0478597),\n",
       " np.float32(0.45296064)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eps_AA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Index  Label_GT  Label_Predit  Constante_Lipschitz  Epsilon_Robuste  \\\n",
      "0      1         6             3             0.350912         0.372034   \n",
      "1      2         6             9             0.191955         0.136486   \n",
      "2      3         8             5             0.192445         0.531047   \n",
      "3      4         7             7             0.536921         0.979334   \n",
      "4      5         9             6             0.710001         0.329805   \n",
      "\n",
      "   Epsilon_Adv_AA  Epsilon_Adv_PGD  \n",
      "0        0.223536         0.717368  \n",
      "1        0.694969         0.661540  \n",
      "2        0.109589         0.149326  \n",
      "3        0.061458         0.299948  \n",
      "4        0.087260         0.351164  \n"
     ]
    }
   ],
   "source": [
    "# penser à tout convertir en numpy\n",
    "total_points = 200\n",
    "\n",
    "# Création du DataFrame avec une colonne d'index de 1 à 200\n",
    "df = pd.DataFrame({\n",
    "    'Index': np.arange(1, total_points + 1),\n",
    "    'Label_GT': labels,  \n",
    "    'Label_Predit': torch.argmax(vanilla_model(images), dim=1),  \n",
    "    'Constante_Lipschitz': np.ones(total_points), \n",
    "    'Epsilon_Robuste': lip_radius,\n",
    "    'Epsilon_Adv_AA': np.random.rand(total_points),\n",
    "    'Epsilon_Adv_PGD': np.random.rand(total_points)\n",
    "})\n",
    "\n",
    "# Affichage des premières lignes du DataFrame\n",
    "print(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "k3torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
